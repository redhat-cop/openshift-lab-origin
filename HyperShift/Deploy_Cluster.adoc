= Deploying Clusters using HyperShift

== Docs

* RHACM 2.5: https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.5/html/clusters/managing-your-clusters#hosted-control-plane-intro
* RHACM 2.6: https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html/multicluster_engine/multicluster_engine_overview#hosted-control-planes-intro

== Prerequisites

* A deployed environment from https://demo.redhat.com[Red Hat Product Demo System (Beta)].
* An understanding of the https://github.com/redhat-cop/openshift-lab-origin/blob/master/HyperShift/Introduction.adoc[Introduction] lab

== Overview

Now that you have an environment with the HyperShift Technology Preview enabled it's time to deploy your first cluster. You will see that is it both easy and fast to create a new cluster using this approach.

Right now the only way to deploy a new cluster is by creating a YAML manifest via the command line - there is no capability in the console yet.

== Deploy a new cluster

In order to deploy a new cluster you need to create a Kubernetes resource of kind `HypershiftDeployment`.

. Log into your Bastion VM if you are not already logged in - the instructions are in the welcome e-mail that you got when you provisioned the environment.
. Create a new YAML file `production.yaml` that will deploy your "production" cluster. It will be similar to the development cluster except that it will be deployed in 2 Zones rather than just one.
+
[source,sh]
----
---
apiVersion: cluster.open-cluster-management.io/v1alpha1
kind: HypershiftDeployment
metadata:
  name: production <1>
  namespace: hypershift
spec:
  hostingCluster: local-cluster
  hostingNamespace: clusters
  hostedClusterSpec:
    networking:
      machineCIDR: 10.0.0.0/16 <2>
      networkType: OVNKubernetes <3>
      podCIDR: 10.132.0.0/14 <2>
      serviceCIDR: 172.31.0.0/16 <2>
    platform:
      type: AWS
    pullSecret:
      name: production-pull-secret <4>
    release:
      image: quay.io/openshift-release-dev/ocp-release:4.11.8-x86_64 <5>
    services:
    - service: APIServer
      servicePublishingStrategy:
        type: LoadBalancer
    - service: OAuthServer
      servicePublishingStrategy:
        type: Route
    - service: Konnectivity
      servicePublishingStrategy:
        type: Route
    - service: Ignition
      servicePublishingStrategy:
        type: Route
    sshKey: {}
  nodePools:
  - name: production-us-west-2b <6>
    spec:
      clusterName: production
      management:
        autoRepair: false
        replace:
          rollingUpdate:
            maxSurge: 1
            maxUnavailable: 0
          strategy: RollingUpdate
        upgradeType: Replace
      platform:
        aws:
          instanceType: m5.large <7>
        type: AWS
      release:
        image: quay.io/openshift-release-dev/ocp-release:4.11.8-x86_64
      replicas: 1
  - name: production-us-west-2c
    spec:
      clusterName: production
      management:
        autoRepair: false
        replace:
          rollingUpdate:
            maxSurge: 1
            maxUnavailable: 0
          strategy: RollingUpdate
        upgradeType: Replace
      platform:
        aws:
          instanceType: m5.large
        type: AWS
      release:
        image: quay.io/openshift-release-dev/ocp-release:4.11.8-x86_64
      replicas: 1
  infrastructure:
    cloudProvider:
      name: aws-credentials <8>
    configure: True
    platform:
      aws:
        region: us-west-2 <9>
        zones: <10>
        - us-west-2b
        - us-west-2c
----
+
[NOTE]
====
<1> Name of the cluster. Referenced a few more times in this resources
<2> Default networking configuration
<3> Networking Plugin to use: `OVNKubernetes` or `OpenShiftSDN`
<4> Name of a secret to store a pull secret. Will be created automatically
<5> OpenShift release image to use
<6> An Array of (worker) node pools. In this example you are distributing your two workers nodes over two different availability zones, us-west-2b and us-west-2c. It is a good idea to use descriptive names for the worker node pools.
<7> Instance type to use for worker nodes (control plane will run as pods on the host cluster)
<8> AWS Credentials and other information that is necessary for deployment (base domain, SSH keys, OpenShift pull secret). The name of the secret - and the secret has to exist in the hub cluster's (`local-cluster` in our case) namespace.
<9> The AWS Region to be deploy the cluster (worker nodes) into.
<10> Optional: which zones to use for the deployment
====

. If you copied the above YAML manifest make sure you didn't copy any of the annotations.
. Create the *production* cluster:
+
[source,sh]
----
oc apply -f production.yaml
----

. Check that the cluster was created successfully:
+
[source,sh]
----
oc get hd -n hypershift
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
NAME          TYPE   INFRA                  IAM                    MANIFESTWORK           PROVIDER REF   PROGRESS    AVAILABLE
development   AWS    ConfiguredAsExpected   ConfiguredAsExpected   ConfiguredAsExpected   AsExpected     Completed   True
production    AWS    BeingConfigured                                                      AsExpected
----

. Repeat the command until you see the following output:
+
[source,sh]
----
oc get hd -n hypershift
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
NAME          TYPE   INFRA                  IAM                    MANIFESTWORK           PROVIDER REF   PROGRESS    AVAILABLE
development   AWS    ConfiguredAsExpected   ConfiguredAsExpected   ConfiguredAsExpected   AsExpected     Completed   True
production    AWS    ConfiguredAsExpected   ConfiguredAsExpected   ConfiguredAsExpected   AsExpected     Partial     False
----
+
That means that your control plane has been configured and the cluster is now deploying the node pools.

. Retrieve the kubeadmin password to access your new cluster and save it to a file in the `$HOME/.kube` directory:
+
[source,sh]
----
oc get secret production-kubeadmin-password -n local-cluster --template='{{ .data.password }}' | base64 -d >$HOME/.kube/production.kubeadmin-password
----

. Retrieve the kukbeconfig file to access your new cluster and save it to a file in the `$HOME/.kube` directory:
+
[source,sh]
----
oc get secret production-admin-kubeconfig -n local-cluster --template='{{ .data.kubeconfig }}' | base64 -d >$HOME/.kube/production-kubeconfig
----

. Set your `KUBECONFIG` variable to use the production cluster configuration:
+
[source,sh]
----
export KUBECONFIG=$HOME/.kube/production-kubeconfig
----

. Check the configuration of the cluster operators:
+
[source,sh]
----
oc get co
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
console
csi-snapshot-controller
dns
image-registry
ingress                                              False       True          True       83s     The "default" ingress controller reports Available=False: IngressControllerUnavailable: One or more status conditions indicate unavailable: DeploymentAvailable=False (DeploymentUnavailable: The deployment has Available status condition set to False (reason: MinimumReplicasUnavailable) with message: Deployment does not have minimum availability.)
insights
kube-apiserver                             4.11.8    True        False         False      2m16s
kube-controller-manager                    4.11.8    True        False         False      2m16s
kube-scheduler                             4.11.8    True        False         False      2m16s
kube-storage-version-migrator
monitoring
network                                    4.11.8    True        True          False      104s    DaemonSet "/openshift-multus/multus" is not yet scheduled on any nodes...
openshift-apiserver                        4.11.8    True        False         False      2m16s
openshift-controller-manager               4.11.8    True        False         False      2m16s
openshift-samples
operator-lifecycle-manager                 4.11.8    True        False         False      2m3s
operator-lifecycle-manager-catalog         4.11.8    True        True          False      2m3s    Deployed 0.19.0
operator-lifecycle-manager-packageserver   4.11.8    True        False         False      2m16s
service-ca
storage
----
+
Depending on how long you waited since you deployed the cluster you may see that some cluster operators are not yet available.

. Check your node pools:
+
[source,sh]
----
oc get nodes
----
+
.Sample Output (No nodes available yet)
[source,texinfo]
----
No resources found
----
+
.Sample Output (Nodes available but not ready yet)
[source,texinfo,options=nowrap]
----
NAME                                         STATUS     ROLES    AGE   VERSION
ip-10-0-137-169.us-west-2.compute.internal   NotReady   worker   11s   v1.24.0+dc5a2fd
ip-10-0-154-205.us-west-2.compute.internal   NotReady   worker   12s   v1.24.0+dc5a2fd
----
+
.Sample Output (Nodes available))
[source,texinfo,options=nowrap]
----
NAME                                         STATUS   ROLES    AGE   VERSION
ip-10-0-137-169.us-west-2.compute.internal   Ready    worker   75s   v1.24.0+dc5a2fd
ip-10-0-154-205.us-west-2.compute.internal   Ready    worker   76s   v1.24.0+dc5a2fd
----
+
Again depending on how long it has been since you created the cluster you may see no nodes, NotReady nodes or you may already see the completely deployed nodes.

. Once the nodes are ready go back and check the Cluster Operators. Repeat this command until the output looks like the one below - this can take a few minutes.
+
[source,sh]
----
oc get co
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
console                                    4.11.8    True        False         False      2m21s
csi-snapshot-controller                    4.11.8    True        False         False      3m33s
dns                                        4.11.8    True        False         False      2m28s
image-registry                             4.11.8    True        False         False      2m2s
ingress                                    4.11.8    True        False         False      2m24s
insights                                   4.11.8    True        False         False      4m13s
kube-apiserver                             4.11.8    True        False         False      8m57s
kube-controller-manager                    4.11.8    True        False         False      8m57s
kube-scheduler                             4.11.8    True        False         False      8m57s
kube-storage-version-migrator              4.11.8    True        False         False      3m41s
monitoring                                 4.11.8    True        False         False      46s
network                                    4.11.8    True        False         False      8m25s
openshift-apiserver                        4.11.8    True        False         False      8m57s
openshift-controller-manager               4.11.8    True        False         False      8m57s
openshift-samples                          4.11.8    True        False         False      2m4s
operator-lifecycle-manager                 4.11.8    True        False         False      8m44s
operator-lifecycle-manager-catalog         4.11.8    True        False         False      8m44s
operator-lifecycle-manager-packageserver   4.11.8    True        False         False      8m57s
service-ca                                 4.11.8    True        False         False      4m13s
storage                                    4.11.8    True        False         False      7s
----

. Retrieve the OpenShift console URL:
+
[source,sh]
----
oc whoami --show-console
----
+
.Sample Output
[source,texinfo]
----
https://console-openshift-console.apps.production.kvrsc.sandbox766.opentlc.com
----

. Open a web browser and use the previously retrieved kubeadmin password to log into the console as `kubeadmin`.
. Explore the Console.

. Once you are done exploring unset the `KUBECONFIG` variable to move back to your hub cluster.
+
[source,sh]
----
unset KUBECONFIG
----

== Summary

This concludes this lab. You have now used Hypershift to deploy a new OpenShift cluster - and you have now seen how quickly you can deploy a new cluster compared to running the OpenShift installer.

= Next steps

Follow https://github.com/redhat-cop/openshift-lab-origin/blob/master/HyperShift/Deploy_Application_.adoc[Deploy an application to HyperShift Clusters] to deploy a micro services application to both HyperShift clusters using Red Hat Advanced Cluster Management for Kubernetes.